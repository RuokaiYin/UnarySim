{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from UnarySim.sw.kernel.nn_utils import *\n",
    "from UnarySim.sw.kernel.linear import UnaryLinear\n",
    "from UnarySim.sw.kernel.relu import UnaryReLU\n",
    "from UnarySim.sw.bitstream.gen import RNG, SourceGen, BSGen\n",
    "from UnarySim.sw.metric.metric import ProgressiveError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\project\\Anaconda3\\Lib\\site-packages\\UnarySim\\sw\\test\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data loader\n",
    "transform=transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=cwd+'/data/mnist', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=cwd+'/data/mnist', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test binary model clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.080000 %\n"
     ]
    }
   ],
   "source": [
    "model_path = cwd+\"\\saved_model_state_dict\"+\"_8_clamp\"\n",
    "model_clamp = MLP3_clamp_eval()\n",
    "model_clamp.to(device)\n",
    "model_clamp.load_state_dict(torch.load(model_path))\n",
    "model_clamp.eval()\n",
    "model_clamp.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model_clamp(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test unary model nonscaled addition - clamp binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 128.23260235786438 seconds ---\n",
      "100 images are done!!!\n",
      "--- 257.59618735313416 seconds ---\n",
      "200 images are done!!!\n",
      "--- 386.40423369407654 seconds ---\n",
      "300 images are done!!!\n",
      "--- 517.1744737625122 seconds ---\n",
      "400 images are done!!!\n",
      "--- 643.6746618747711 seconds ---\n",
      "500 images are done!!!\n",
      "--- 769.734424829483 seconds ---\n",
      "600 images are done!!!\n",
      "--- 901.5013029575348 seconds ---\n",
      "700 images are done!!!\n",
      "--- 1033.2616741657257 seconds ---\n",
      "800 images are done!!!\n",
      "--- 1161.9022915363312 seconds ---\n",
      "900 images are done!!!\n",
      "--- 1291.920536994934 seconds ---\n",
      "1000 images are done!!!\n",
      "--- 1418.7784140110016 seconds ---\n",
      "1100 images are done!!!\n",
      "--- 1548.3510174751282 seconds ---\n",
      "1200 images are done!!!\n",
      "--- 1678.38414311409 seconds ---\n",
      "1300 images are done!!!\n",
      "--- 1805.9496567249298 seconds ---\n",
      "1400 images are done!!!\n",
      "--- 1940.1211307048798 seconds ---\n",
      "1500 images are done!!!\n",
      "--- 2068.0224800109863 seconds ---\n",
      "1600 images are done!!!\n",
      "--- 2195.2713956832886 seconds ---\n",
      "1700 images are done!!!\n",
      "--- 2328.4279408454895 seconds ---\n",
      "1800 images are done!!!\n",
      "--- 2464.318659543991 seconds ---\n",
      "1900 images are done!!!\n",
      "--- 2594.1222281455994 seconds ---\n",
      "2000 images are done!!!\n",
      "--- 2723.964406967163 seconds ---\n",
      "2100 images are done!!!\n",
      "--- 2856.459095478058 seconds ---\n",
      "2200 images are done!!!\n",
      "--- 2984.330805540085 seconds ---\n",
      "2300 images are done!!!\n",
      "--- 3113.4399003982544 seconds ---\n",
      "2400 images are done!!!\n",
      "--- 3242.6810386180878 seconds ---\n",
      "2500 images are done!!!\n",
      "--- 3370.3111896514893 seconds ---\n",
      "2600 images are done!!!\n",
      "--- 3497.8668971061707 seconds ---\n",
      "2700 images are done!!!\n",
      "--- 3626.2625184059143 seconds ---\n",
      "2800 images are done!!!\n",
      "--- 3754.7375082969666 seconds ---\n",
      "2900 images are done!!!\n",
      "--- 3886.271404981613 seconds ---\n",
      "3000 images are done!!!\n",
      "--- 4014.2007236480713 seconds ---\n",
      "3100 images are done!!!\n",
      "--- 4141.8396372795105 seconds ---\n",
      "3200 images are done!!!\n",
      "--- 4270.752272605896 seconds ---\n",
      "3300 images are done!!!\n",
      "--- 4398.331839323044 seconds ---\n",
      "3400 images are done!!!\n",
      "--- 4525.646770954132 seconds ---\n",
      "3500 images are done!!!\n",
      "--- 4654.4659123420715 seconds ---\n",
      "3600 images are done!!!\n",
      "--- 4785.954065084457 seconds ---\n",
      "3700 images are done!!!\n",
      "--- 4918.905645608902 seconds ---\n",
      "3800 images are done!!!\n",
      "--- 5047.404773712158 seconds ---\n",
      "3900 images are done!!!\n",
      "--- 5178.329708576202 seconds ---\n",
      "4000 images are done!!!\n",
      "--- 5303.6268100738525 seconds ---\n",
      "4100 images are done!!!\n",
      "--- 5431.082888126373 seconds ---\n",
      "4200 images are done!!!\n",
      "--- 5558.3401210308075 seconds ---\n",
      "4300 images are done!!!\n",
      "--- 5685.955498218536 seconds ---\n",
      "4400 images are done!!!\n",
      "--- 5815.781569957733 seconds ---\n",
      "4500 images are done!!!\n",
      "--- 5942.681703329086 seconds ---\n",
      "4600 images are done!!!\n",
      "--- 6070.591898918152 seconds ---\n",
      "4700 images are done!!!\n",
      "--- 6200.3009440898895 seconds ---\n",
      "4800 images are done!!!\n",
      "--- 6327.867136001587 seconds ---\n",
      "4900 images are done!!!\n",
      "--- 6457.191490650177 seconds ---\n",
      "5000 images are done!!!\n",
      "--- 6583.249600172043 seconds ---\n",
      "5100 images are done!!!\n",
      "--- 6710.893448114395 seconds ---\n",
      "5200 images are done!!!\n",
      "--- 6838.593876361847 seconds ---\n",
      "5300 images are done!!!\n",
      "--- 6969.400940418243 seconds ---\n",
      "5400 images are done!!!\n",
      "--- 7096.069935321808 seconds ---\n",
      "5500 images are done!!!\n",
      "--- 7224.380739212036 seconds ---\n",
      "5600 images are done!!!\n",
      "--- 7355.7013027668 seconds ---\n",
      "5700 images are done!!!\n",
      "--- 7483.166594028473 seconds ---\n",
      "5800 images are done!!!\n",
      "--- 7611.062618255615 seconds ---\n",
      "5900 images are done!!!\n",
      "--- 7739.665743350983 seconds ---\n",
      "6000 images are done!!!\n",
      "--- 7867.293705463409 seconds ---\n",
      "6100 images are done!!!\n",
      "--- 7994.77131819725 seconds ---\n",
      "6200 images are done!!!\n",
      "--- 8120.916974544525 seconds ---\n",
      "6300 images are done!!!\n",
      "--- 8247.729614257812 seconds ---\n",
      "6400 images are done!!!\n",
      "--- 8379.748416423798 seconds ---\n",
      "6500 images are done!!!\n",
      "--- 8511.086011648178 seconds ---\n",
      "6600 images are done!!!\n",
      "--- 8637.24723815918 seconds ---\n",
      "6700 images are done!!!\n",
      "--- 8768.083844184875 seconds ---\n",
      "6800 images are done!!!\n",
      "--- 8898.108781337738 seconds ---\n",
      "6900 images are done!!!\n",
      "--- 9026.591937065125 seconds ---\n",
      "7000 images are done!!!\n",
      "--- 9155.755009889603 seconds ---\n",
      "7100 images are done!!!\n",
      "--- 9284.018300294876 seconds ---\n",
      "7200 images are done!!!\n",
      "--- 9412.8804500103 seconds ---\n",
      "7300 images are done!!!\n",
      "--- 9539.72437787056 seconds ---\n",
      "7400 images are done!!!\n"
     ]
    }
   ],
   "source": [
    "correct_binary = 0\n",
    "correct_unary = 0\n",
    "\n",
    "bitwidth = 8\n",
    "total = 0\n",
    "\n",
    "# binary MLP3_clamp weight init\n",
    "rng = \"Sobol\"\n",
    "rng_dim = 1\n",
    "relu_buf_dep = 4\n",
    "mode = \"bipolar\"\n",
    "scaled = False\n",
    "bias = True\n",
    "sample_cnt = 50000\n",
    "\n",
    "start_cnt = 0\n",
    "current_index = 0\n",
    "\n",
    "cycle_correct = torch.zeros(2**(bitwidth)).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        if current_index < start_cnt:\n",
    "            current_index = current_index + 1\n",
    "            continue\n",
    "        current_index = current_index + 1\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # reference binary mlp\n",
    "        outputs_binary = model_clamp(images)\n",
    "        _, predicted_binary = torch.max(outputs_binary.data, 1)\n",
    "        correct_binary += (predicted_binary == labels).sum().item()\n",
    "        \n",
    "#         print(model_clamp.fc1_out.min().item(), model_clamp.fc1_out.max().item())\n",
    "#         print(model_clamp.fc2_out.min().item(), model_clamp.fc2_out.max().item())\n",
    "#         print(model_clamp.fc3_out.min().item(), model_clamp.fc3_out.max().item())\n",
    "\n",
    "\n",
    "        # unary part\n",
    "        # input image check\n",
    "        image = images.view(-1, 32*32)\n",
    "        image_SRC = SourceGen(image, bitwidth=bitwidth, mode=mode)().to(device)\n",
    "        image_RNG = RNG(bitwidth, rng_dim, rng)().to(device)\n",
    "        image_BSG = BSGen(image_SRC, image_RNG).to(device)\n",
    "        image_ERR = ProgressiveError(image, mode=mode).to(device)\n",
    "        \n",
    "        # unary mlp is decomposed into separate layers\n",
    "        fc1_unary = UnaryLinear(32*32, 512, model_clamp.fc1.weight.data, model_clamp.fc1.bias.data, \n",
    "                        mode=mode, scaled=scaled, bias=bias).to(device)\n",
    "        fc1_ERR = ProgressiveError(model_clamp.fc1_out, mode=mode).to(device)\n",
    "        \n",
    "        fc2_unary = UnaryLinear(512, 512, model_clamp.fc2.weight.data, model_clamp.fc2.bias.data, \n",
    "                                mode=mode, scaled=scaled, bias=bias).to(device)\n",
    "        fc2_ERR = ProgressiveError(model_clamp.fc2_out, mode=mode).to(device)\n",
    "\n",
    "        fc3_unary = UnaryLinear(512, 10, model_clamp.fc3.weight.data, model_clamp.fc3.bias.data, \n",
    "                                mode=mode, scaled=scaled, bias=bias).to(device)\n",
    "        fc3_ERR = ProgressiveError(model_clamp.fc3_out, mode=mode).to(device)\n",
    "        \n",
    "        relu1_unary = UnaryReLU(buf_dep=relu_buf_dep, bitwidth=bitwidth, rng=rng).to(device)\n",
    "        relu1_ERR = ProgressiveError(model_clamp.relu1_out, mode=mode).to(device)\n",
    "        \n",
    "        relu2_unary = UnaryReLU(buf_dep=relu_buf_dep, bitwidth=bitwidth, rng=rng).to(device)\n",
    "        relu2_ERR = ProgressiveError(model_clamp.relu2_out, mode=mode).to(device)\n",
    "        \n",
    "        if total%100 == 0:\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            print(total, \"images are done!!!\")\n",
    "\n",
    "#         print(current_index, \"-th image with label\", labels.item(), \", total image count\", total)\n",
    "        for i in range(2**(bitwidth)):\n",
    "            idx = torch.zeros(image_SRC.size()).type(torch.long).to(device)\n",
    "            image_bs = image_BSG(idx + i)\n",
    "            image_ERR.Monitor(image_bs)\n",
    "            # print(image_bs.shape)\n",
    "            # fc1\n",
    "            fc1_unary_out   = fc1_unary(image_bs)\n",
    "#             fc1_ERR.Monitor(fc1_unary_out)\n",
    "            # print(fc1_unary_out.shape)\n",
    "            # relu1\n",
    "            relu1_unary_out = relu1_unary(fc1_unary_out)\n",
    "#             relu1_ERR.Monitor(relu1_unary_out)\n",
    "            # print(relu1_unary_out.shape)\n",
    "            # fc2\n",
    "            fc2_unary_out   = fc2_unary(relu1_unary_out)\n",
    "#             fc2_ERR.Monitor(fc2_unary_out)\n",
    "            # print(fc2_unary_out.shape)\n",
    "            # relu2\n",
    "            relu2_unary_out = relu2_unary(fc2_unary_out)\n",
    "#             relu2_ERR.Monitor(relu2_unary_out)\n",
    "            # print(relu2_unary_out.shape)\n",
    "            # fc3\n",
    "            fc3_unary_out   = fc3_unary(relu2_unary_out)\n",
    "            fc3_ERR.Monitor(fc3_unary_out)\n",
    "            # print(fc3_unary_out.shape)\n",
    "            \n",
    "            _, predicted_unary = torch.max(fc3_ERR()[0], 1)\n",
    "            if predicted_unary == labels:\n",
    "#                 print(current_index, \"-th image succeeds.\")\n",
    "#                 print(current_index, \"-th image with label\", labels.item(), \", total image count\", total)\n",
    "#                 print(\"before\", predicted_unary.item(), cycle_correct[predicted_unary.item()].item())\n",
    "                cycle_correct[i].add_(1)\n",
    "#                 print(\"after\", predicted_unary.item(), cycle_correct[predicted_unary.item()].item())\n",
    "\n",
    "#         to_print = 1\n",
    "#         print(\"image: \", \n",
    "#               image_ERR()[to_print].min().item(), \n",
    "#               image_ERR()[to_print].max().item(),\n",
    "#               image_ERR()[to_print].mul(image_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc1:   \", \n",
    "#               fc1_ERR()[to_print].min().item(), \n",
    "#               fc1_ERR()[to_print].max().item(), \n",
    "#               fc1_ERR()[to_print].mul(fc1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"relu1: \", \n",
    "#               relu1_ERR()[to_print].min().item(), \n",
    "#               relu1_ERR()[to_print].max().item(), \n",
    "#               relu1_ERR()[to_print].mul(relu1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc2:   \", \n",
    "#               fc2_ERR()[to_print].min().item(), \n",
    "#               fc2_ERR()[to_print].max().item(), \n",
    "#               fc2_ERR()[to_print].mul(fc2_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"relu1: \", \n",
    "#               relu2_ERR()[to_print].min().item(), \n",
    "#               relu2_ERR()[to_print].max().item(), \n",
    "#               relu1_ERR()[to_print].mul(relu1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc3:   \", \n",
    "#               fc3_ERR()[to_print].min().item(), \n",
    "#               fc3_ERR()[to_print].max().item(), \n",
    "#               fc3_ERR()[to_print].mul(fc3_ERR()[to_print]).mean().sqrt().item())\n",
    "        \n",
    "        _, predicted_unary = torch.max(fc3_ERR()[0], 1)\n",
    "        correct_unary += (predicted_unary == labels).sum().item()\n",
    "        if total == sample_cnt:\n",
    "            break\n",
    "\n",
    "print('Accuracy of the network on %d test images: %f %%' % (total,\n",
    "    100 * correct_binary / total))\n",
    "print('Accuracy of the network on %d test images: %f %%' % (total,\n",
    "    100 * correct_unary / total))\n",
    "\n",
    "result = cycle_correct.cpu().numpy()/total\n",
    "fig = plt.plot([i for i in range(2**bitwidth)], result)  # arguments are passed to np.histogram\n",
    "plt.title(\"Cycle level accuracy\")\n",
    "plt.show()\n",
    "\n",
    "with open(\"cycle_accuracy_mlp_nonscaled_clamp.csv\", \"w+\") as f:\n",
    "    for i in result:\n",
    "        f.write(str(i)+\", \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
