{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from UnarySim.sw.kernel.nn_utils import *\n",
    "from UnarySim.sw.kernel.linear import GainesLinear\n",
    "from UnarySim.sw.kernel.relu import UnaryReLU\n",
    "from UnarySim.sw.bitstream.gen import RNG, SourceGen, BSGen\n",
    "from UnarySim.sw.metric.metric import ProgressiveError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\project\\Anaconda3\\Lib\\site-packages\\UnarySim\\sw\\test\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data loader\n",
    "transform=transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=cwd+'/data/mnist', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=cwd+'/data/mnist', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test binary model clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.080000 %\n"
     ]
    }
   ],
   "source": [
    "model_path = cwd+\"\\saved_model_state_dict_8_clamp\"\n",
    "model_clamp = MLP3_clamp_eval()\n",
    "model_clamp.to(device)\n",
    "model_clamp.load_state_dict(torch.load(model_path))\n",
    "model_clamp.eval()\n",
    "model_clamp.to(device)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model_clamp(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test unary model nonscaled addition - clamp binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10 test images: 90.000000 %\n",
      "Accuracy of the network on 10 test images: 10.000000 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVeElEQVR4nO3cf7RdZX3n8feHxGDxBxAIFkJqYqGrK44zCJegUxdDFwIJHQkt2AJTDdZKxymrWqcdsc4qNradsUsrOotaGcWCI1LUUuOaMpGKTqug5sLwKyASMJIYlGD4VRgHYr7zx9lXj7f35p4kNznc+7xfa52Vc579PHs/37vvOp+zn31uUlVIktqz37AnIEkaDgNAkhplAEhSowwASWqUASBJjTIAJKlRBoBmjCQbk7x6N8adlGTz3pjTuONUkqP29nGk6WIAaK9Icl6S0ST/lOTBJNcledWw5yXpxwwATbskbwMuAf4UeBHwM8BfACuHOa+WJZkz7Dno2ccA0LRKciCwGvjtqvqbqnqyqp6pqs9V1e8n+ekkTyU5pG/McUm2JnlO9/pNSe5O8kSSu5IcO8Fx9ktyUZL7knw/yTVJ5g84xyOSfKY75reS/E5f+//t30+Slyd5uG9uv9HN7ZEka5O8eMBjvqGvpvuT/Na47SuT3Jrk8a6m5V37/CQfS7KlO+bfdu3nJ/nyuH38aAkqyV8l+VCSv0vyJPCLSX4pyf/pjrEpybvGjX9VkhuTPNptPz/J8Um+l2RuX7+zktw6SN16djMANN1eCTwXuHaijVX1XeBLwK/2Nf86cHVVPZPktcC7gNcDLwTOAL4/wa5+BzgT+DfAEcAjwKVTTS7JfsDngNuAhcDJwFuTnFZVW4CbgLP6hpwHfLqb25nAHwC/AiwA/hH45FTH7DwE/NuupjcA7x8LtiTLgCuB3wcOAk4ENnbjPg4cALwUOAx4/4DHG5v7nwAvAL4MPEnv53oQ8EvAm7uaSPIzwHXAf+tqOwa4tarW0fv5n9K331/v5qWZrqp8+Ji2B/DvgO9O0efXgK90z+cA3wWWda/XAm+ZZNxG4NXd87uBk/u2HQ48A8ydYNxJwObu+QnAA+O2vwP4WPf8N4EbuucBNgEndq+vA97YN24/4Cngxd3rAo4a8Of0t2N1Ah8G3j9Bn8OBHcDBE2w7H/jyuLYfHR/4K+DKKeZwydhxu5/BtZP0ezvwie75/K7mw4f9u+Zjzx9eAWi6fR84tH/JYAKfBZYmeQm9T5aPVdXXu22LgPsGOM6LgWu75YpH6QXCD+ndc5hq3BFj47qxf9A37tPAK5McQe+TeNH7pD829gN947bRC4mFU002yYokX02yrRt7OnDoFDUvArZV1SNT7X8Sm8bN4YQkX+yWvh4D/v0AcwD4H8Brkjyf3pXbP1bVg7s5Jz2LGACabjcBP6C3PDOhqvoBcA29q4XX8ZPLCZuAnx3gOJuAFVV1UN/juVX1nQHGfWvcuBdU1end3B4FPk/vje484JNVVX1jf2vc2J+qqht3dsAk+wOfAd4LvKiqDgL+jl547KzmTcD8JAdNsO1JektDY8f46Qn6jP+vfq8C1gCLqupA4C8HmAPdz/Qm4Jf55+dLM5gBoGlVVY8BfwhcmuTMJAckeU73CfjP+rpeSW8Z4wx6nzDHfAT4ve7GcJIcNcmN1r8E/mRsW5IFSQb5ltHXgceTvD3JTyWZk+RfJDm+r89V9NbKz+qe9x/zHUle2h3zwO6exVTmAfsDW4HtSVYAp/Zt/yjwhiQndze3Fyb5+e5T9nXAXyQ5uPs5ntiNuQ14aZJjkjyX3n2TqbyA3hXFD7r7Duf1bfsE8Ookv5pkbpJDkhzTt/1K4D8BL2OS+zuaeQwATbuq+nPgbcB/pvemtwm4kN6691ifr9Bb376lqjb2tX+K3o3Lq4AnujETfbvnA/Q+zX4+yRPAV+mt7081tx8Cr6F3k/NbwMP0QufAvm5rgKOB71XVbX1jrwXeA1yd5HHgTmDFAMd8gt5N62vo3aw+rzvG2Pav090YBh4D/je95SbofeJ+BvgGvRvJb+3GfJPet63+HriX3k3eqfwHYHX38/rDbj5jc3iA3rLUf6S3tHUr8K/6xl7bzenaqnpygGNpBsiPr26lfSvJDcBVVfWRYc9FU0tyH70lsL8f9lw0PXZ2o07aa7oll2Pxj8NmhCRn0buncMOw56LpYwBon0tyBb2bxG/plkf0LJbkS8BS4HVVtWPI09E0cglIkhrlTWBJatSMWgI69NBDa/HixcOehiTNKDfffPPDVbVgfPuMCoDFixczOjo67GlI0oyS5NsTtbsEJEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNGigAkixPck+SDUkummD7iUluSbI9ydnjtq1Kcm/3WDXB2DVJ7tz9EiRJu2PKAEgyB7gUWAEsBc5NsnRctweA84Grxo2dD1wMnAAsAy5OcnDf9l8B/mkP5i9J2k2DXAEsAzZU1f1V9TRwNbCyv0NVbayq24Ed48aeBlxfVduq6hHgemA5QJLnA28D/ngPa5Ak7YZBAmAhsKnv9eaubRA7G/tu4H3AUzvbQZILkowmGd26deuAh5UkTWWQAMgEbTXg/iccm+QY4KiqunaqHVTVZVU1UlUjCxYsGPCwkqSpDBIAm4FFfa+PBLYMuP/Jxr4SOC7JRuDLwM8l+dKA+5QkTYNBAmAdcHSSJUnmAecAawbc/1rg1CQHdzd/TwXWVtWHquqIqloMvAr4ZlWdtOvTlyTtrikDoKq2AxfSezO/G7imqtYnWZ3kDIAkxyfZDLwW+HCS9d3YbfTW+td1j9VdmyRpyFI16HL+8I2MjNTo6OiwpyFJM0qSm6tqZHy7fwksSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGjVQACRZnuSeJBuSXDTB9hOT3JJke5Kzx21bleTe7rGqazsgyf9M8o0k65P81+kpR5I0qCkDIMkc4FJgBbAUODfJ0nHdHgDOB64aN3Y+cDFwArAMuDjJwd3m91bVzwMvB34hyYo9qEOStIsGuQJYBmyoqvur6mngamBlf4eq2lhVtwM7xo09Dbi+qrZV1SPA9cDyqnqqqr7YjX0auAU4cg9rkSTtgkECYCGwqe/15q5tEFOOTXIQ8BrgCwPuU5I0DQYJgEzQVgPuf6djk8wFPgl8sKrun3AHyQVJRpOMbt26dcDDSpKmMkgAbAYW9b0+Etgy4P6nGnsZcG9VXTLZDqrqsqoaqaqRBQsWDHhYSdJUBgmAdcDRSZYkmQecA6wZcP9rgVOTHNzd/D21ayPJHwMHAm/d9WlLkvbUlAFQVduBC+m9cd8NXFNV65OsTnIGQJLjk2wGXgt8OMn6buw24N30QmQdsLqqtiU5EngnvW8V3ZLk1iS/uRfqkyRNIlWDLucP38jISI2Ojg57GpI0oyS5uapGxrf7l8CS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDVqoABIsjzJPUk2JLlogu0nJrklyfYkZ4/btirJvd1jVV/7cUnu6Pb5wSTZ83IkSYOaMgCSzAEuBVYAS4Fzkywd1+0B4HzgqnFj5wMXAycAy4CLkxzcbf4QcAFwdPdYvttVSJJ22dwB+iwDNlTV/QBJrgZWAneNdaiqjd22HePGngZcX1Xbuu3XA8uTfAl4YVXd1LVfCZwJXLcnxUzmjz63nru2PL43di1Je93SI17Ixa956bTvd5AloIXApr7Xm7u2QUw2dmH3fMp9JrkgyWiS0a1btw54WEnSVAa5Aphobb4G3P9kYwfeZ1VdBlwGMDIyMuhxf8LeSE5JmukGuQLYDCzqe30ksGXA/U82dnP3fHf2KUmaBoMEwDrg6CRLkswDzgHWDLj/tcCpSQ7ubv6eCqytqgeBJ5K8ovv2z+uBz+7G/CVJu2nKAKiq7cCF9N7M7wauqar1SVYnOQMgyfFJNgOvBT6cZH03dhvwbnohsg5YPXZDGHgz8BFgA3Afe+kGsCRpYqnarWX1oRgZGanR0dFhT0OSZpQkN1fVyPh2/xJYkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNWqgAEiyPMk9STYkuWiC7fsn+etu+9eSLO7a5yX5WJI7ktyW5KS+Med27bcn+V9JDp2mmiRJA5gyAJLMAS4FVgBLgXOTLB3X7Y3AI1V1FPB+4D1d+5sAquplwCnA+5Lsl2Qu8AHgF6vqXwK3AxdOQz2SpAENcgWwDNhQVfdX1dPA1cDKcX1WAld0zz8NnJwk9ALjCwBV9RDwKDACpHs8r+v3QmDLHtYiSdoFgwTAQmBT3+vNXduEfapqO/AYcAhwG7AyydwkS4DjgEVV9QzwZuAOem/8S4GP7kEdkqRdNEgAZIK2GrDP5fQCYxS4BLgR2J7kOfQC4OXAEfSWgN4x4cGTC5KMJhndunXrANOVJA1ikADYDCzqe30k/3y55kd9uvX9A4FtVbW9qn63qo6pqpXAQcC9wDEAVXVfVRVwDfCvJzp4VV1WVSNVNbJgwYJdKE2StDODBMA64OgkS5LMA84B1ozrswZY1T0/G7ihqirJAUmeB5DkFGB7Vd0FfAdYmmTsHf0U4O49rEWStAvmTtWhqrYnuRBYC8wBLq+q9UlWA6NVtYbe+v3Hk2wAttELCYDDgLVJdtB7039dt88tSf4I+IckzwDfBs6f3tIkSTuT3grMzDAyMlKjo6PDnoYkzShJbq6qkfHt/iWwJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNGigAkixPck+SDUkummD7/kn+utv+tSSLu/Z5ST6W5I4ktyU5qW/MvCSXJflmkm8kOWuaapIkDWDuVB2SzAEuBU4BNgPrkqypqrv6ur0ReKSqjkpyDvAe4NeANwFU1cuSHAZcl+T4qtoBvBN4qKp+Lsl+wPxprUyStFODXAEsAzZU1f1V9TRwNbByXJ+VwBXd808DJycJsBT4AkBVPQQ8Cox0/X4D+C/dth1V9fCeFCJJ2jWDBMBCYFPf681d24R9qmo78BhwCHAbsDLJ3CRLgOOARUkO6sa9O8ktST6V5EUTHTzJBUlGk4xu3bp14MIkSTs3SABkgrYasM/l9AJjFLgEuBHYTm/p6UjgK1V1LHAT8N6JDl5Vl1XVSFWNLFiwYIDpSpIGMeU9AHpv4Iv6Xh8JbJmkz+Ykc4EDgW1VVcDvjnVKciNwL/B94Cng2m7Tp+jdR5Ak7SODXAGsA45OsiTJPOAcYM24PmuAVd3zs4EbqqqSHJDkeQBJTgG2V9VdXTB8DjipG3MycBeSpH1myiuAqtqe5EJgLTAHuLyq1idZDYxW1Rrgo8DHk2wAttELCYDDgLVJdgDfAV7Xt+u3d2MuAbYCb5iuoiRJU0vvw/jMMDIyUqOjo8OehiTNKElurqqR8e3+JbAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRqaphz2FgSbYC397N4YcCD0/jdJ7tWqq3pVrBemezvVXri6tqwfjGGRUAeyLJaFWNDHse+0pL9bZUK1jvbLava3UJSJIaZQBIUqNaCoDLhj2BfayleluqFax3NtuntTZzD0CS9JNaugKQJPUxACSpUbM+AJIsT3JPkg1JLhr2fPaGJBuT3JHk1iSjXdv8JNcnubf79+Bhz3N3Jbk8yUNJ7uxrm7C+9HywO9+3Jzl2eDPfPZPU+64k3+nO8a1JTu/b9o6u3nuSnDacWe+eJIuSfDHJ3UnWJ3lL1z4rz+9O6h3O+a2qWfsA5gD3AS8B5gG3AUuHPa+9UOdG4NBxbX8GXNQ9vwh4z7DnuQf1nQgcC9w5VX3A6cB1QIBXAF8b9vynqd53Ab83Qd+l3e/1/sCS7vd9zrBr2IVaDweO7Z6/APhmV9OsPL87qXco53e2XwEsAzZU1f1V9TRwNbByyHPaV1YCV3TPrwDOHOJc9khV/QOwbVzzZPWtBK6snq8CByU5fN/MdHpMUu9kVgJXV9X/q6pvARvo/d7PCFX1YFXd0j1/ArgbWMgsPb87qXcye/X8zvYAWAhs6nu9mZ3/sGeqAj6f5OYkF3RtL6qqB6H3SwccNrTZ7R2T1Tebz/mF3bLH5X1LerOm3iSLgZcDX6OB8zuuXhjC+Z3tAZAJ2mbj915/oaqOBVYAv53kxGFPaIhm6zn/EPCzwDHAg8D7uvZZUW+S5wOfAd5aVY/vrOsEbbOh3qGc39keAJuBRX2vjwS2DGkue01Vben+fQi4lt4l4vfGLo27fx8a3gz3isnqm5XnvKq+V1U/rKodwH/nx8sAM77eJM+h92b4iar6m6551p7fieod1vmd7QGwDjg6yZIk84BzgDVDntO0SvK8JC8Yew6cCtxJr85VXbdVwGeHM8O9ZrL61gCv774t8grgsbGlhJls3Dr3L9M7x9Cr95wk+ydZAhwNfH1fz293JQnwUeDuqvrzvk2z8vxOVu/Qzu+w74rvg7vup9O7034f8M5hz2cv1PcSet8SuA1YP1YjcAjwBeDe7t/5w57rHtT4SXqXxc/Q+0T0xsnqo3fJfGl3vu8ARoY9/2mq9+NdPbd3bwqH9/V/Z1fvPcCKYc9/F2t9Fb0ljduBW7vH6bP1/O6k3qGcX/8rCElq1GxfApIkTcIAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY36/5v8evN8U7TRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct_binary = 0\n",
    "correct_unary = 0\n",
    "\n",
    "bitwidth = 8\n",
    "total = 0\n",
    "\n",
    "# binary MLP3_clamp weight init\n",
    "rng = \"Sobol\"\n",
    "rng_dim = 1\n",
    "relu_buf_dep = 4\n",
    "mode = \"bipolar\"\n",
    "scaled = False\n",
    "bias = True\n",
    "sample_cnt = 10\n",
    "\n",
    "start_cnt = 0\n",
    "current_index = 0\n",
    "\n",
    "cycle_correct = torch.zeros(2**(bitwidth)).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        if current_index < start_cnt:\n",
    "            current_index = current_index + 1\n",
    "            continue\n",
    "        current_index = current_index + 1\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # reference binary mlp\n",
    "        outputs_binary = model_clamp(images)\n",
    "        _, predicted_binary = torch.max(outputs_binary.data, 1)\n",
    "        correct_binary += (predicted_binary == labels).sum().item()\n",
    "        \n",
    "#         print(model_clamp.fc1_out.min().item(), model_clamp.fc1_out.max().item())\n",
    "#         print(model_clamp.fc2_out.min().item(), model_clamp.fc2_out.max().item())\n",
    "#         print(model_clamp.fc3_out.min().item(), model_clamp.fc3_out.max().item())\n",
    "\n",
    "\n",
    "        # unary part\n",
    "        # input image check\n",
    "        image = images.view(-1, 32*32)\n",
    "        image_SRC = SourceGen(image, bitwidth=bitwidth, mode=mode)().to(device)\n",
    "        image_RNG = RNG(bitwidth, rng_dim, rng)().to(device)\n",
    "        image_BSG = BSGen(image_SRC, image_RNG).to(device)\n",
    "        image_ERR = ProgressiveError(image, mode=mode).to(device)\n",
    "        \n",
    "        # unary mlp is decomposed into separate layers\n",
    "        fc1_unary = GainesLinear(32*32, 512, model_clamp.fc1.weight.data, model_clamp.fc1.bias.data, \n",
    "                                 mode=mode, scaled=scaled, bias=bias, depth=bitwidth).to(device)\n",
    "        fc1_ERR = ProgressiveError(model_clamp.fc1_out, mode=mode).to(device)\n",
    "        \n",
    "        fc2_unary = GainesLinear(512, 512, model_clamp.fc2.weight.data, model_clamp.fc2.bias.data, \n",
    "                                 mode=mode, scaled=scaled, bias=bias, depth=bitwidth).to(device)\n",
    "        fc2_ERR = ProgressiveError(model_clamp.fc2_out, mode=mode).to(device)\n",
    "\n",
    "        fc3_unary = GainesLinear(512, 10, model_clamp.fc3.weight.data, model_clamp.fc3.bias.data, \n",
    "                                 mode=mode, scaled=scaled, bias=bias, depth=bitwidth).to(device)\n",
    "        fc3_ERR = ProgressiveError(model_clamp.fc3_out, mode=mode).to(device)\n",
    "        \n",
    "        relu1_unary = UnaryReLU(buf_dep=relu_buf_dep, bitwidth=bitwidth, rng=rng).to(device)\n",
    "        relu1_ERR = ProgressiveError(model_clamp.relu1_out, mode=mode).to(device)\n",
    "        \n",
    "        relu2_unary = UnaryReLU(buf_dep=relu_buf_dep, bitwidth=bitwidth, rng=rng).to(device)\n",
    "        relu2_ERR = ProgressiveError(model_clamp.relu2_out, mode=mode).to(device)\n",
    "        \n",
    "        if total%100 == 0:\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            print(total, \"images are done!!!\")\n",
    "\n",
    "#         print(current_index, \"-th image with label\", labels.item(), \", total image count\", total)\n",
    "        for i in range(2**(bitwidth)):\n",
    "            idx = torch.zeros(image_SRC.size()).type(torch.long).to(device)\n",
    "            image_bs = image_BSG(idx + i)\n",
    "            image_ERR.Monitor(image_bs)\n",
    "            # print(image_bs.shape)\n",
    "            # fc1\n",
    "            fc1_unary_out   = fc1_unary(image_bs)\n",
    "#             fc1_ERR.Monitor(fc1_unary_out)\n",
    "            # print(fc1_unary_out.shape)\n",
    "            # relu1\n",
    "            relu1_unary_out = relu1_unary(fc1_unary_out)\n",
    "#             relu1_ERR.Monitor(relu1_unary_out)\n",
    "            # print(relu1_unary_out.shape)\n",
    "            # fc2\n",
    "            fc2_unary_out   = fc2_unary(relu1_unary_out)\n",
    "#             fc2_ERR.Monitor(fc2_unary_out)\n",
    "            # print(fc2_unary_out.shape)\n",
    "            # relu2\n",
    "            relu2_unary_out = relu2_unary(fc2_unary_out)\n",
    "#             relu2_ERR.Monitor(relu2_unary_out)\n",
    "            # print(relu2_unary_out.shape)\n",
    "            # fc3\n",
    "            fc3_unary_out   = fc3_unary(relu2_unary_out)\n",
    "            fc3_ERR.Monitor(fc3_unary_out)\n",
    "            # print(fc3_unary_out.shape)\n",
    "            \n",
    "            _, predicted_unary = torch.max(fc3_ERR()[0], 1)\n",
    "            if predicted_unary == labels:\n",
    "#                 print(current_index, \"-th image succeeds.\")\n",
    "#                 print(current_index, \"-th image with label\", labels.item(), \", total image count\", total)\n",
    "#                 print(\"before\", predicted_unary.item(), cycle_correct[predicted_unary.item()].item())\n",
    "                cycle_correct[i].add_(1)\n",
    "#                 print(\"after\", predicted_unary.item(), cycle_correct[predicted_unary.item()].item())\n",
    "\n",
    "#         to_print = 1\n",
    "#         print(\"image: \", \n",
    "#               image_ERR()[to_print].min().item(), \n",
    "#               image_ERR()[to_print].max().item(),\n",
    "#               image_ERR()[to_print].mul(image_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc1:   \", \n",
    "#               fc1_ERR()[to_print].min().item(), \n",
    "#               fc1_ERR()[to_print].max().item(), \n",
    "#               fc1_ERR()[to_print].mul(fc1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"relu1: \", \n",
    "#               relu1_ERR()[to_print].min().item(), \n",
    "#               relu1_ERR()[to_print].max().item(), \n",
    "#               relu1_ERR()[to_print].mul(relu1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc2:   \", \n",
    "#               fc2_ERR()[to_print].min().item(), \n",
    "#               fc2_ERR()[to_print].max().item(), \n",
    "#               fc2_ERR()[to_print].mul(fc2_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"relu1: \", \n",
    "#               relu2_ERR()[to_print].min().item(), \n",
    "#               relu2_ERR()[to_print].max().item(), \n",
    "#               relu1_ERR()[to_print].mul(relu1_ERR()[to_print]).mean().sqrt().item())\n",
    "#         print(\"fc3:   \", \n",
    "#               fc3_ERR()[to_print].min().item(), \n",
    "#               fc3_ERR()[to_print].max().item(), \n",
    "#               fc3_ERR()[to_print].mul(fc3_ERR()[to_print]).mean().sqrt().item())\n",
    "        \n",
    "        _, predicted_unary = torch.max(fc3_ERR()[0], 1)\n",
    "        correct_unary += (predicted_unary == labels).sum().item()\n",
    "        if total == sample_cnt:\n",
    "            break\n",
    "\n",
    "print('Accuracy of the network on %d test images: %f %%' % (total,\n",
    "    100 * correct_binary / total))\n",
    "print('Accuracy of the network on %d test images: %f %%' % (total,\n",
    "    100 * correct_unary / total))\n",
    "\n",
    "result = cycle_correct.cpu().numpy()/total\n",
    "fig = plt.plot([i for i in range(2**bitwidth)], result)  # arguments are passed to np.histogram\n",
    "plt.title(\"Cycle level accuracy\")\n",
    "plt.show()\n",
    "\n",
    "with open(\"cycle_accuracy_mlp_nonscaled_clamp_gaines.csv\", \"w+\") as f:\n",
    "    for i in result:\n",
    "        f.write(str(i)+\", \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
